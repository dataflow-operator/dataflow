apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: kc-iceberg-titan-stocks
  namespace: kafka-connect
  labels:
    strimzi.io/cluster: kc-iceberg-sink
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  class: io.tabular.iceberg.connect.IcebergSinkConnector
  tasksMax: 1
  config:
    # === Catalog & Authentication ===
    iceberg.catalog.ref: main
    iceberg.catalog.type: nessie
    iceberg.table.format: parquet
    iceberg.catalog.uri: http://nessie.nessie:19120/api/v2
    iceberg.catalog.authentication.type: BEARER
    iceberg.catalog.authentication.token: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:bearer-token}
    iceberg.catalog.io-impl: org.apache.iceberg.aws.s3.S3FileIO
    iceberg.catalog.s3.access-key-id: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:s3-access-key}
    iceberg.catalog.s3.secret-access-key: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:s3-secret-key}
    iceberg.catalog.s3.endpoint: https://storage.yandexcloud.net
    iceberg.catalog.s3.path-style-access: 'true'
    iceberg.tables: titan_raw.stocks
    iceberg.tables.auto-create-enabled: 'false'
    iceberg.control.commit.interval-ms: '60000'
    iceberg.object-store-layout.enabled: 'true'
    iceberg.tables.evolve-schema-enabled: 'true'
    iceberg.tables.schema-case-insensitive: 'true'
    iceberg.control.topic: com-data-platform-int-control-iceberg-0
    iceberg.catalog.warehouse: s3a://com-data-platform-prod1-data-lake-prod1

    # === Kafka Topics ===
    key.converter.schemas.enable: false
    value.converter.schemas.enable: true
    topics: com-titan-int-stock-import-prod

    # === Key Converter
    key.converter: org.apache.kafka.connect.storage.StringConverter

    # === Value Converter тут дублируем параметры из KafkaConnect иначе не взлетит
    value.converter: io.confluent.connect.avro.AvroConverter
    value.converter.basic.auth.credentials.source: USER_INFO
    value.converter.basic.auth.user.info: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:basic_auth}
    value.converter.schema.registry.url: https://rc1a-hd87l57gvr84oggg.mdb.yandexcloud.net,https://rc1b-tp6jn81lehb8cmfa.mdb.yandexcloud.net,https://rc1d-q4m8e90ianbtej5p.mdb.yandexcloud.net

    # === Transforms ===
    transforms: insertTimestamp
    transforms.insertTimestamp.type: org.apache.kafka.connect.transforms.InsertField$Value
    transforms.insertTimestamp.timestamp.field: kafka_message_timestamp

    # transforms: extractHeaderField
    # transforms.extractHeaderField.type: org.apache.kafka.connect.transforms.HeaderFrom$Value
    # transforms.extractHeaderField.headers: timestamp
    # transforms.extractHeaderField.fields: kafka_message_timestamp
    # transforms.extractHeaderField.operation: copy

    # errors.tolerance: none
    errors.tolerance: all
    errors.log.enable: true
    errors.log.include.messages: true
