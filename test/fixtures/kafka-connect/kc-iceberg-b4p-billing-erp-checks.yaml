apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: kc-iceberg-b4p-billing-erp-checks
  namespace: kafka-connect
  labels:
    strimzi.io/cluster:  kc-iceberg-sink-prod01
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  class: io.tabular.iceberg.connect.IcebergSinkConnector
  tasksMax: 1
  config:
    # === Catalog & Authentication ===
    # Конфигурация каталога Nessie для хранения метаданных Iceberg таблиц
    iceberg.catalog.ref: stage
    iceberg.catalog.type: nessie
    iceberg.table.format: parquet
    iceberg.catalog.uri: http://nessie.nessie:19120/api/v2
    iceberg.catalog.authentication.type: BEARER
    iceberg.catalog.authentication.token: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:bearer-token}

    # === S3 Storage Configuration ===
    # Настройки для работы с Yandex Object Storage (S3-совместимое хранилище)
    iceberg.catalog.io-impl: org.apache.iceberg.aws.s3.S3FileIO
    iceberg.catalog.s3.access-key-id: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:s3-access-key}
    iceberg.catalog.s3.secret-access-key: ${secrets:kafka-connect/kc-kafka-cred-tp-prod-env-vars:s3-secret-key}
    iceberg.catalog.s3.endpoint: https://storage.yandexcloud.net
    iceberg.catalog.s3.path-style-access: 'true'
    iceberg.catalog.warehouse: s3a://com-data-platform-prod1-data-lake-prod1

    # === Iceberg Tables Configuration ===
    # Таблица для хранения данных чеков из ERP системы биллинга
    iceberg.tables: check_raw.checks
    iceberg.tables.auto-create-enabled: 'false'
    iceberg.control.commit.interval-ms: '60000'
    iceberg.object-store-layout.enabled: 'true'
    iceberg.tables.evolve-schema-enabled: 'true'
    iceberg.tables.schema-case-insensitive: 'true'
    iceberg.control.topic: com-data-platform-int-control-iceberg-0-prod01

    # === Kafka Topics ===
    # Топик с данными чеков из ERP системы биллинга (B4P)
    key.converter.schemas.enable: false
    value.converter.schemas.enable: true
    topics: b4p-billing-erp-check-prod

    # === Key Converter ===
    # Ключи сообщений в формате строки
    key.converter: org.apache.kafka.connect.storage.StringConverter

    # === Value Converter ===
    # Значения сообщений в формате Avro с использованием Schema Registry
    # Параметры дублируются из KafkaConnect для корректной работы
    value.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter.schemas.enable: false

    # === Transforms ===
    # Добавление timestamp получения сообщения из Kafka в данные
    transforms: insertTimestamp
    transforms.insertTimestamp.type: org.apache.kafka.connect.transforms.InsertField$Value
    transforms.insertTimestamp.timestamp.field: kafka_message_timestamp

    # === Error Handling ===
    # Режим обработки ошибок: продолжать работу при ошибках с логированием
    errors.tolerance: all
    errors.log.enable: true
    errors.log.include.messages: true
